{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1_7TmttFE14"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ALnACsXFIuI"
      },
      "source": [
        "# Text Analysis Assignment\n",
        "\n",
        "## Objective\n",
        "The goal of this assignment is to assess your ability to perform text analysis using Python libraries, focusing on regular expressions and other text processing techniques.\n",
        "\n",
        "## Dataset\n",
        "For this assignment, we will use the [Twitter Sentiment Analysis Dataset](https://www.kaggle.com/kazanova/sentiment140).  \n",
        "\n",
        "Note: This lab will not work on older versions of Python; make sure to work on Googe colab\n",
        "\n",
        "**This assignment should be completed and submitted by 11:59 PM PST on Friday, Oct 25th, 2024**\n",
        "\n",
        "## Step 1: Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPmq_crXFZto",
        "outputId": "3ab45a12-1c34-4d6c-8eec-1ceaca1b9b02"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import zipfile\n",
        "import requests\n",
        "\n",
        "# Download NLTK resources if needed\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Lzd3qHdFkKE"
      },
      "source": [
        "### Step 2: Load the Dataset from URL\n",
        "\n",
        "\n",
        "*   First, we need to download and extract the dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhIXtH52ArlI",
        "outputId": "0020412e-a62d-4422-9960-bccf28d22dcc"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'requests' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Download and extract the dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Save the zip file locally\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmsspamcollection.zip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
            "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined"
          ]
        }
      ],
      "source": [
        "# Download and extract the dataset\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Save the zip file locally\n",
        "with open('smsspamcollection.zip', 'wb') as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile('smsspamcollection.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "# Load the dataset into a DataFrame (the file name is 'SMSSpamCollection')\n",
        "df = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['label', 'message'])\n",
        "\n",
        "# Preview the data\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5IBHbLxF4lE"
      },
      "source": [
        "## Question 1: Data Overview\n",
        "Display the shape of the dataset and check for any missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mbL_PR7gF9Hb"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Answer here\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Display the shape of the dataset and check for any missing values\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "#Answer here\n",
        "# Display the shape of the dataset and check for any missing values\n",
        "print(f\"Shape of the dataset: {df.shape}\")\n",
        "print(f\"Missing values:\\n{df.isnull().sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AEJJDP-GBOL"
      },
      "source": [
        "## Question 2: Data Cleaning\n",
        "Write a function to clean the text data by removing URLs and converting text to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq1x91XhGUQc"
      },
      "outputs": [],
      "source": [
        "# Function to clean the text data\n",
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Apply the cleaning function to the 'message' column\n",
        "df['cleaned_message'] = df['message'].apply(clean_text)\n",
        "\n",
        "# Preview cleaned data\n",
        "print(df[['message', 'cleaned_message']].head())\n",
        "#Answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGng_P59GOt3"
      },
      "source": [
        "### Question 3: Tokenization\n",
        "Tokenize the cleaned text into words and display the first five tokens from one entry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxRu7TUoGA7F"
      },
      "outputs": [],
      "source": [
        "# Tokenize the cleaned text into words\n",
        "df['tokenized_message'] = df['cleaned_message'].apply(word_tokenize)\n",
        "\n",
        "# Display the first five tokens from one entry\n",
        "print(df['tokenized_message'].iloc[0][:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0LGP8e5GWu1"
      },
      "source": [
        "### Question 4: Removing Stop Words\n",
        "Remove common stop words from the tokenized words and display the remaining words for one entry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8amW6nfGa08"
      },
      "outputs": [],
      "source": [
        "# Remove stop words from tokenized words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "df['no_stopwords_message'] = df['tokenized_message'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "\n",
        "# Display remaining words for one entry\n",
        "print(df['no_stopwords_message'].iloc[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdZQxU7sGc_m"
      },
      "source": [
        "### Question 5: Word Frequency Distribution\n",
        "1. Create a frequency distribution of words in the cleaned text and plot it using Matplotlib.\n",
        "2. Focus on displaying only the top 10 most common words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv8nod_2Gg0L"
      },
      "outputs": [],
      "source": [
        "# Flatten the list of words for the entire dataset\n",
        "all_words = [word for tokens in df['no_stopwords_message'] for word in tokens]\n",
        "\n",
        "# Create frequency distribution\n",
        "freq_dist = nltk.FreqDist(all_words)\n",
        "\n",
        "# Plot the top 10 most common words\n",
        "common_words = freq_dist.most_common(10)\n",
        "words, counts = zip(*common_words)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(words, counts)\n",
        "plt.title('Top 10 Most Common Words')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcBSHXCSGjq_"
      },
      "source": [
        "### Question 6: Label Distribution\n",
        "1. Analyze the distribution of spam and ham messages in the dataset. Create a bar plot to visualize the counts of each label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x65qwde8Gi2D"
      },
      "outputs": [],
      "source": [
        "# Analyze the distribution of spam and ham messages\n",
        "label_counts = df['label'].value_counts()\n",
        "\n",
        "# Create a bar plot to visualize the counts of each label\n",
        "plt.figure(figsize=(7, 5))\n",
        "label_counts.plot(kind='bar', color=['blue', 'orange'])\n",
        "plt.title('Distribution of Spam vs Ham Messages')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zb3uynNzGrrx"
      },
      "source": [
        "### Question 7: Length of Messages\n",
        "1. Calculate the length of each message (in terms of character count) and add this as a new column to the DataFrame. Then, visualize the distribution of message lengths using a histogram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaYbv_twGrIf"
      },
      "outputs": [],
      "source": [
        "# Calculate the length of each message\n",
        "df['message_length'] = df['message'].apply(len)\n",
        "\n",
        "# Visualize the distribution of message lengths using a histogram\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(df['message_length'], bins=20, color='green')\n",
        "plt.title('Distribution of Message Lengths')\n",
        "plt.xlabel('Message Length (Characters)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgoumF37Gycl"
      },
      "source": [
        "### Question 8: Most Common Words in Spam vs. Ham\n",
        "1. Identify and display the most common words in spam messages compared to ham messages. Use a simple frequency count for this analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsoWTKbFG2ed"
      },
      "outputs": [],
      "source": [
        "# Separate spam and ham messages\n",
        "spam_words = [word for tokens in df[df['label'] == 'spam']['no_stopwords_message'] for word in tokens]\n",
        "ham_words = [word for tokens in df[df['label'] == 'ham']['no_stopwords_message'] for word in tokens]\n",
        "\n",
        "# Create frequency distributions for spam and ham words\n",
        "spam_freq_dist = nltk.FreqDist(spam_words)\n",
        "ham_freq_dist = nltk.FreqDist(ham_words)\n",
        "\n",
        "# Display the most common words in spam and ham messages\n",
        "print(\"Most common words in spam messages:\", spam_freq_dist.most_common(5))\n",
        "print(\"Most common words in ham messages:\", ham_freq_dist.most_common(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d5GH6NdG6kC"
      },
      "source": [
        "### Question 9: Word Cloud Visualization\n",
        "1. Create a word cloud for the cleaned text of all messages. This visualization will help you see which words are most prominent in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkfFzCJBG4Dq"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "# Combine all the cleaned messages into one large string\n",
        "all_text = ' '.join(df['cleaned_message'])\n",
        "\n",
        "# Generate word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm5T54-zG-Gu"
      },
      "source": [
        "### Question 10: Sentiment Analysis (Basic)\n",
        "1. Perform a basic sentiment analysis by checking for specific keywords (e.g., \"free\", \"win\", \"call\", etc.) in spam messages.\n",
        "2. Count how many spam messages contain at least one of these keywords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Yzmc3smG5Qh"
      },
      "outputs": [],
      "source": [
        "# Define spam-related keywords\n",
        "keywords = [\"free\", \"win\", \"call\", \"prize\", \"money\", \"urgent\"]\n",
        "\n",
        "# Count messages that contain at least one of these keywords\n",
        "spam_messages_containing_keywords = [msg for msg in df[df['label'] == 'spam']['cleaned_message'] if any(keyword in msg.lower() for keyword in keywords)]\n",
        "spam_keyword_count = len(spam_messages_containing_keywords)\n",
        "\n",
        "print(f\"Number of spam messages containing keywords: {spam_keyword_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjmweZD4HB8Q"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myu1QRRUE-1A"
      },
      "source": [
        "### Reflection on Findings:\n",
        "1. Reflect on what you learned from this assignment.\n",
        "2. Discuss any insights gained from analyzing spam versus ham messages, including patterns you observed or challenges you faced during your analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6emgwoaI5du"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzD7VzofHXdX"
      },
      "outputs": [],
      "source": [
        "From this assignment, I have learned the importance of preprocessing steps such as cleaning, tokenization, and stopword removal in text analysis. By analyzing the data, I found that spam messages often contain certain keywords such as \"free\", \"win\", and \"prize\", which can be used for simple sentiment analysis. \n",
        "\n",
        "I also observed that spam messages tend to be more promotional and action-oriented, while ham messages are more conversational. One of the challenges was dealing with noisy data, including special characters and varying formats, but regular expressions and careful text cleaning made the analysis smoother.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9smYc7IGHdue"
      },
      "source": [
        "### Submission\n",
        "1. Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output.  \n",
        "2. name your noteboob with your Roll number(e.g 2022cs_01_assignment2)\n",
        "3. Upload your notebook and make sure to turn in before dealine."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
